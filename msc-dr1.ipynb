{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4950975,"sourceType":"datasetVersion","datasetId":2871088},{"sourceId":5043891,"sourceType":"datasetVersion","datasetId":2928021},{"sourceId":6520374,"sourceType":"datasetVersion","datasetId":3769556},{"sourceId":7066223,"sourceType":"datasetVersion","datasetId":4068788}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"train_x_list = \"/kaggle/input/wisdm-data/wisdm/x_train.npy\"\ntrain_y_list = \"/kaggle/input/wisdm-data/wisdm/y_train.npy\"\ntest_x_list = \"/kaggle/input/wisdm-data/wisdm/x_test.npy\"\ntest_y_list = \"/kaggle/input/wisdm-data/wisdm/y_test.npy\"\ntrain_x_list = \"/kaggle/input/opportunity/OPPORTUNITY/x_train.npy\"\ntrain_y_list = \"/kaggle/input/opportunity/OPPORTUNITY/y_train.npy\"\ntest_x_list = \"/kaggle/input/opportunity/OPPORTUNITY/x_test.npy\"\ntest_y_list = \"/kaggle/input/opportunity/OPPORTUNITY/y_test.npy\"\ntrain_x_list = \"/kaggle/input/sb-zby/x_train.npy\"\ntrain_y_list = \"/kaggle/input/opportunity/OPPORTUNITY/y_train.npy\"\ntest_x_list = \"/kaggle/input/opportunity/OPPORTUNITY/x_test.npy\"\ntest_y_list = \"/kaggle/input/opportunity/OPPORTUNITY/y_test.npy\"\nimport datetime\nimport os\nimport csv\nimport numpy as np\nimport random\nimport shutil\nimport torch\nimport argparse\nfrom torch.cuda.amp import autocast as autocast\nfrom torch.cuda.amp import GradScaler\nimport os\n\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nimport torch.utils.data as Data\nfrom collections import Counter\nfrom imblearn.over_sampling import BorderlineSMOTE\n\nfrom sklearn.model_selection import train_test_split\n\nimport argparse\n\nparser = argparse.ArgumentParser()\n\nimport torch\n\nparser = argparse.ArgumentParser(description=\"Experiment Info and Setings, Model Hyperparameters\")\nparser.add_argument(\"--lambda_cls\", type=float, default=1)\nparser.add_argument(\"--lambda_sc\", type=float, default=2)\nparser.add_argument(\"--lambda_st\", type=float, default=0.2)\nparser.add_argument(\"--lambda_cos_loss\", type=float, default=2)\n# Experiment Info\nparser.add_argument(\"--experiment_date\", type=str, default=f\"{datetime.datetime.now().strftime('%Y%m%d')}\")\nparser.add_argument(\"--experiment_time\", type=str, default=f\"{datetime.datetime.now().strftime('%H:%M:%S')}\")\nparser.add_argument(\"--characteristic\", '-c', type=str, default=\"\")\nparser.add_argument(\"--data\", type=str, default='Sleep-edf')\nparser.add_argument(\"--data_type\", type=str, default='epoch')\nparser.add_argument(\"--scheme\", type=str, default='M_M')\nparser.add_argument(\"--loss_weight\", type=int, default=1)\nparser.add_argument(\"--lstm_layers\", type=int, default=1)\nparser.add_argument(\"--cos_loss\", type=int, default=1)\nparser.add_argument(\"--mha\", type=int, default=1)\nparser.add_argument(\"--mha_length\", type=int, default=8)\nparser.add_argument(\"--mha_head\", type=int, default=2)\nparser.add_argument(\"--mass_ch\", type=str, default='eeg_f4-ler')\nparser.add_argument(\"--downsample\", type=int, default=100)\n# Experiment Hyperparameters\nparser.add_argument(\"--epoch\", type=int, default=150)\nparser.add_argument(\"--lr\", type=float, default=1e-3)\nparser.add_argument(\"--wd\", type=float, default=1e-3)\n\nparser.add_argument(\"--early_stop\", type=int, default=50)\nparser.add_argument(\"--dropout\", type=int, default=0.5)\nparser.add_argument(\"--scheduler\", type=int, default=0)\nparser.add_argument(\"--stride\", type=str, default=2)\nparser.add_argument(\"--preprocess\", type=str, default='robustscale')\n# Model Hyperparameters\nparser.add_argument(\"--batch\", type=int, default=64)\nparser.add_argument(\"--seq_length\", type=int, default=4)\n# GPU\nparser.add_argument(\"--GPU\", type=bool, default=True)\nparser.add_argument(\"--gpu_idx\", type=int, default=-1)\n# Experiment Sbj\nparser.add_argument(\"--range_start\", type=int, default=0)\nparser.add_argument(\"--range_end\", type=int, default=31)\nargs = parser.parse_args(args=[])\n#args = parser.parse_known_args()[0]\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch import default_generator  # type: ignore\nfrom typing import Tuple\nfrom torch import Tensor, Generator\nimport mne\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset\nfrom numpy.random import shuffle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def record_name(cv_i):\n    # return args.characteristic + '_Finetuning_Model_' + args.finetuning_model_name + str(args.lr_finetune_network)\n    return args.characteristic + '_CV_' + str(cv_i)\n\n\ndef writelog(file, line):\n    with open(file, 'a', encoding='utf-8') as f:\n        f.write(line + '\\n')\n    print(line + '\\n')\n\ndef label_stage_transition(label_list):\n    for l in range(len(label_list)):\n        label_shape = label_list[l].shape\n        label = label_list[l].flatten()\n        lbl = np.zeros_like(label)\n        for i in range(1, len(label)):\n            if i != len(label) - 1:\n                if label[i] == label[i - 1] and label[i] == label[i + 1]:\n                    lbl[i] = 0\n                else:\n                    lbl[i] = 1\n            else:\n                if label[i] == label[i - 1]:\n                    lbl[i] = 0\n                else:\n                    lbl[i] = 1\n        #cls, count = np.unique(lbl, return_counts=True)\n        #writelog(log_file, f'Lable Count: {dict(zip(cls, count))}')\n        lbl = lbl.reshape(label_shape)\n        label_list[l] = lbl\n    return label_list\n\ndef float_tensor(x):\n    return torch.FloatTensor(x)\n\ndef long_tensor(x):\n    return torch.LongTensor(x)\n\ndef dcn(x):  # detach, cpu, numpy\n    if type(x) == np.ndarray:\n        return x\n    else:\n        return x.detach().cpu().numpy()\n\ndef pb_argmax(x):\n    if len(x.shape) == 1:\n        return (x)\n    else:\n        return np.argmax(x, axis=-1)\n\ndef flatten_1dim(x):\n    if len(x.shape) == 1:\n        return (x)\n    else:\n        return x.flatten()\n\ndef flatten_logit_list(l):\n    for i in range(len(l)):\n        l[i] = l[i].flatten(end_dim=1)\n    return l\n\ndef standardize(x):\n    return (x - x.mean(axis=1)[:, None]) / x.std(axis=1)[:, None]\n\ndef downsample_to_100(x):\n    x = mne.filter.resample(x, down=2.56, axis=-1)\n    return x\n\ndef tr_val_split(idx_tr, idx_val, X_tr_val, Y_tr_val):\n    return X_tr_val[idx_tr], Y_tr_val[idx_tr], X_tr_val[idx_val], Y_tr_val[idx_val]\n\ndef pytorch_sliding_window(x, window_size, step_size=1):\n    # Unfold Dimension to Make Slding Window\n    return x.unfold(0, window_size, step_size)\n\n''' Count Parameters '''\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n''' Loss '''\n\ndef loss_cross_entropy(weight=None, reduction='mean'):  # Take Logit as an Input\n    return nn.CrossEntropyLoss(weight=weight, reduction=reduction)\n\ndef loss_cos_loss(margin=0, reduction='mean'):\n    return nn.CosineEmbeddingLoss(margin=margin, reduction=reduction)\n\ndef loss_mse(reduction='none'):\n    return torch.nn.MSELoss(reduction=reduction)\n\ndef loss_calculate(y_hat, y, y_pre=None, loss_type=None, ignore_index=None, regularizer_const=None, step=None):\n    loss = loss_type(y_hat, y)\n    return loss\n\ndef loss_weight_balance(label):\n    '''\n    Qu et al., (JBHI, 2020)\n    '''\n    label, count = np.unique(label, return_counts=True)\n    ratio_reciprocal = np.reciprocal(count / count.sum())\n    loss_weight = ratio_reciprocal * (len(label) / (ratio_reciprocal.sum()))  # Weight Sum = Num of Label\n    return loss_weight\n\n\n''' Optimizer '''\n\ndef optimizer(params, name='network'):\n    if name == 'network':\n        lr = args.lr_network\n    elif name == 'rss':\n        lr = args.lr_rss\n    opt = Adam(params, lr=lr)\n    # lr_scedule = lr_scheduler.ExponentialLR(opt, gamma=0.96)\n    return opt\n\n\nclass Optimizer():\n    def __init__(self, network):\n        super(Optimizer, self).__init__()\n        self.opt = torch.optim.Adam(network.parameters(),\n                                    lr=args.lr, weight_decay=args.wd)\n\n    def opt_zero_grad(self):\n        self.opt.zero_grad()\n\n    def opt_step(self):\n        self.opt.step()\n\ndef tensor_form(X: list, Y: list):\n    for i in range(len(X)):\n        X[i] = float_tensor(X[i])\n    for i in range(len(Y)):\n        Y[i] = long_tensor((Y[i]))\n    return X, Y\n\nclass tensordataset_w_indices(Dataset[Tuple[Tensor, ...]]):\n    r\"\"\" *** Custom ***\n    Dataset wrapping tensors.\n    Each sample will be retrieved by indexing tensors along the first dimension.\n    Args:\n        *tensors (Tensor): tensors that have the same size of the first dimension.\n    \"\"\"\n    tensors: Tuple[Tensor, ...]\n\n    def __init__(self, *tensors: Tensor) -> None:\n        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors), \"Size mismatch between tensors\"\n        self.tensors = tensors\n\n    def __getitem__(self, index):\n        # (X, Y), idx\n        return tuple(tensor[index] for tensor in self.tensors), index\n\n    def __len__(self):\n        return self.tensors[0].size(0)\n\ndef dataloader_form(X_tr, Y_tr, X_val, Y_val, X_ts, Y_ts):\n    tr_loader = DataLoader(tensordataset_w_indices(X_tr, Y_tr), batch_size=args.batch, shuffle=True, pin_memory=True)\n    val_loader = DataLoader(TensorDataset(X_val, Y_val), batch_size=args.batch, pin_memory=True)\n    ts_loader = DataLoader(TensorDataset(X_ts, Y_ts), batch_size=args.batch, pin_memory=True)\n    return tr_loader, val_loader, ts_loader\n\ndef predict(dataloader, network):\n    Y_new, Y_hat, Y_hat_pb = np.array([]), np.array([]), np.array([[], [], [], [], [], []]).reshape(0, 6)\n    v_t_correct = 0\n    v_t_total = 0\n    test_loss = 0.0\n    test_correct = 0\n    test_total = 0\n    y_true = []\n    y_pred = []\n    for iteration, batch in enumerate(zip(dataloader)):\n        x, y = batch[0]\n        x, y = x.to(device), y.flatten().to(device)\n\n        with torch.no_grad():\n            l_1, l_2, l_2_t = network(x)\n            loss = l_2\n            _, predicted = torch.max(x.data, 1)\n            test_correct += (predicted == y).sum().item()\n            test_total += y.size(0)\n            # 累计测试损失.\n            test_loss += loss.item()\n\n    # 计算测试准确率和损失.\n    test_acc = 100.0 * test_correct / test_total\n    test_loss = test_loss / len(dataloader)\n\n    # 打印测试结果.\n    print('Test Loss: {:.4f}, Test Acc: {:.2f}%'.format(test_loss, test_acc))\n    # 返回测试结果.\n    return test_loss, test_acc\n\nif (torch.cuda.is_available()):\n    device = torch.device(\"cuda\")\n    print(\"使用GPU训练中：{}\".format(torch.cuda.get_device_name()))\nelse:\n    device = torch.device(\"cpu\")\n    print(\"使用CPU训练\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class HAR_BorderlineSMOTE(Data.Dataset):\n    def __init__(self, filename_x, filename_y):\n        self.filename_x = filename_x\n        self.filename_y = filename_y\n\n    def HAR_data(self):\n        data_x_raw = np.load(self.filename_x)\n        data_x = data_x_raw\n        data_y = np.load(self.filename_y)\n        data_x = torch.tensor(data_x, dtype=torch.float32)\n        data_y = torch.tensor(data_y, dtype=torch.long)\n        smo = BorderlineSMOTE(random_state=42, kind=\"borderline-1\")\n        n, nx, ny = data_x.shape\n        data_x = data_x.reshape((n, nx * ny))\n        data_x, data_y = smo.fit_resample(data_x, data_y)\n        data_x = data_x.reshape((data_x.shape[0], nx, ny))\n\n        print(Counter(data_y))\n        train_data, val_data, train_label, val_label = train_test_split(data_x, data_y, test_size=0.1, random_state=42)\n\n        torch_dataset = []\n        torch_dataset = Data.TensorDataset(torch.from_numpy(train_data),\n                                           torch.from_numpy(train_label)), Data.TensorDataset(\n            torch.from_numpy(val_data), torch.from_numpy(val_label))\n        return torch_dataset\n\nclass trian_HAR(Data.Dataset):\n    def __init__(self, filename_x, filename_y):\n        self.filename_x = filename_x\n        self.filename_y = filename_y\n\n    def HAR_data(self):\n        data_x_raw = np.load(self.filename_x)\n\n        data_x = data_x_raw  # (N, C, H, W) (7352, 1, 128, 9)\n        # data_x = np.expand_dims(data_x_raw, 1)\n        data_y = np.load(self.filename_y)\n\n        data_x = data_x.transpose(0, 2, 1)\n        print(\"data_x.shape:\", data_x.shape)\n        data_x = X_window_maker(data_x)\n        data_y = Y_window_maker(data_y)\n        print(\"data_x.shape:\", data_x.shape)\n        print(\"data_y.shape:\", data_y.shape)\n        data_x = torch.tensor(data_x, dtype=torch.float32)\n        data_y = torch.tensor(data_y, dtype=torch.long)\n        train_data, val_data, train_label, val_label = train_test_split(data_x, data_y, test_size=0.1, random_state=42)\n        #print(val_label.type)\n        #train_dataset,val_dataset = tensordataset_w_indices(train_data, train_label), Data.TensorDataset(val_data, val_label)\n\n        return train_data, train_label, val_data, val_label\n\nclass HAR(Data.Dataset):\n    def __init__(self, filename_x, filename_y):\n        self.filename_x = filename_x\n        self.filename_y = filename_y\n\n    def HAR_data(self):\n        data_x_raw = np.load(self.filename_x)\n        data_y = np.load(self.filename_y)\n        data_x = data_x_raw  # (N, C, H, W) (7352, 1, 128, 9)\n        data_x = data_x.transpose(0, 2, 1)\n        data_x = X_window_maker(data_x)\n        data_y = Y_window_maker(data_y)\n        data_x = torch.tensor(data_x, dtype=torch.float32)\n        data_y = torch.tensor(data_y, dtype=torch.long)\n        # data_x = np.expand_dims(data_x_raw, 1)\n        #print(data_y.type)\n\n        #torch_dataset = Data.TensorDataset(data_x, data_y)\n        return data_x, data_y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def X_window_maker(x):\n    remain = x.shape[0] % args.seq_length  # Sliding Window and Permute\n    x_window = x[remain:].reshape(-1, args.seq_length, x.shape[-1] * x.shape[-2])  # Slice The Remained From Front\n    return x_window\n\n\ndef Y_window_maker(y):\n    remain = y.shape[0] % args.seq_length\n    y_window = y[remain:].reshape(-1, args.seq_length)\n    return y_window\n\nimport numpy as np\n\n#数据上采样部分\n#data_train = HAR(train_x_list, train_y_list)\n#data_train = HAR_SMOTE(train_x_list, train_y_list)\ndata_train = trian_HAR(train_x_list, train_y_list)\ntrain_data, train_y, val_data, val_y = data_train.HAR_data()\ndata_test = HAR(test_x_list, test_y_list)\ntest_data, test_y = data_test.HAR_data()\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import robust_scale\nfrom mne.filter import filter_data\n\ndef robustscaler(x_tr, x_val, x_ts): \n    scaler = RobustScaler()\n    x_tr_sh = x_tr.shape\n    x_val_sh = x_val.shape\n    x_ts_sh = x_ts.shape\n    x_tr = scaler.fit_transform(x_tr.reshape(-1, x_tr_sh[-1]))\n    x_val = scaler.transform(x_val.reshape(-1, x_val_sh[-1]))\n    x_ts = scaler.transform(x_ts.reshape(-1, x_ts_sh[-1]))\n    x_tr = x_tr.reshape(*x_tr_sh)\n    x_val = x_val.reshape(*x_val_sh)\n    x_ts = x_ts.reshape(*x_ts_sh)\n    return x_tr, x_val, x_ts\n\n\nX_tr, X_val, X_ts = robustscaler(train_data, val_data, test_data)\n[X_tr, X_val, X_ts], [Y_tr_org, Y_val_org, Y_ts_org] = tensor_form([X_tr, X_val, X_ts], [train_y, val_y, test_y])\nY_tr_t, Y_val_t, Y_ts_t = label_stage_transition([Y_tr_org, Y_val_org, Y_ts_org])\n[], [Y_tr_t, Y_val_t, Y_ts_t] = tensor_form([], [Y_tr_t, Y_val_t, Y_ts_t])\nloss_weight_org = float_tensor(loss_weight_balance(Y_tr_org)).to(device)  # tensor array\nloss_weight_t = float_tensor(loss_weight_balance(Y_tr_t)).to(device)  # tensor array\nloss_weight_dict = {'org': loss_weight_org, 'trans': loss_weight_t}\n\ntr_loader, val_loader, ts_loader = dataloader_form(X_tr, Y_tr_org, X_val, Y_val_org, X_ts, Y_ts_org)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\n#from retention import MultiScaleRetention\nimport math\nimport torch\nimport torch.nn as nn\ndevice = torch.device(\"cuda\")\ndef fixed_pos_embedding(x):\n    seq_len, dim = x.shape\n    inv_freq = 1.0 / (10000 ** (torch.arange(0, dim) / dim))\n    sinusoid_inp = (\n        torch.einsum(\"i , j -> i j\", torch.arange(0, seq_len, dtype=torch.float), inv_freq).to(x)\n    )\n    return torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)\n\ndef rotate_every_two(x):\n    x1 = x[:, :, ::2]\n    x2 = x[:, :, 1::2]\n\n    x = torch.stack((-x2, x1), dim=-1)\n    return x.flatten(-2)\n\n    # in einsum notation: rearrange(x, '... d j -> ... (d j)')\\\n\ndef duplicate_interleave(m):\n    \"\"\"\n    A simple version of `torch.repeat_interleave` for duplicating a matrix while interleaving the copy.\n    \"\"\"\n    dim0 = m.shape[0]\n    m = m.view(-1, 1)  # flatten the matrix\n    m = m.repeat(1, 2)  # repeat all elements into the 2nd dimension\n    m = m.view(dim0, -1)  # reshape into a matrix, interleaving the copy\n    return m\n\ndef apply_rotary_pos_emb(x, sin, cos, scale=1):\n    sin, cos = map(lambda t: duplicate_interleave(t * scale), (sin, cos))\n    # einsum notation for lambda t: repeat(t[offset:x.shape[1]+offset,:], \"n d -> () n () (d j)\", j=2)\n    return (x * cos) + (rotate_every_two(x) * sin)\n\nclass XPOS(nn.Module):\n    def __init__(\n            self, head_dim, scale_base=512\n    ):\n        super().__init__()\n        self.head_dim = head_dim\n        self.scale_base = scale_base\n        self.register_buffer(\n            \"scale\", (torch.arange(0, head_dim, 2) + 0.4 * head_dim) / (1.4 * head_dim)\n        )\n\n    def forward(self, x, offset=0, downscale=False):\n        length = x.shape[1]\n        min_pos = 0\n        max_pos = length + offset + min_pos\n        scale = self.scale ** torch.arange(min_pos, max_pos, 1).to(self.scale).div(self.scale_base)[:, None]\n        sin, cos = fixed_pos_embedding(scale)\n\n        if scale.shape[0] > length:\n            scale = scale[-length:]\n            sin = sin[-length:]\n            cos = cos[-length:]\n\n        if downscale:\n            scale = 1 / scale\n\n        x = apply_rotary_pos_emb(x, sin, cos, scale)\n        return x\n\n    def forward_reverse(self, x, offset=0, downscale=False):\n        length = x.shape[1]\n        min_pos = -(length + offset) // 2\n        max_pos = length + offset + min_pos\n        scale = self.scale ** torch.arange(min_pos, max_pos, 1).to(self.scale).div(self.scale_base)[:, None]\n        sin, cos = fixed_pos_embedding(scale)\n\n        if scale.shape[0] > length:\n            scale = scale[-length:]\n            sin = sin[-length:]\n            cos = cos[-length:]\n\n        if downscale:\n            scale = 1 / scale\n\n        x = apply_rotary_pos_emb(x, -sin, cos, scale)\n        return x\nclass SimpleRetention(nn.Module):\n    def __init__(self, hidden_size, gamma, head_size=None, double_v_dim=False):\n        \"\"\"\n        Simple retention mechanism based on the paper\n        \"Retentive Network: A Successor to Transformer for Large Language Models\"[https://arxiv.org/pdf/2307.08621.pdf]\n        \"\"\"\n        super(SimpleRetention, self).__init__()\n\n        self.hidden_size = hidden_size\n        if head_size is None:\n            head_size = hidden_size\n        self.head_size = head_size\n\n        self.v_dim = head_size * 2 if double_v_dim else head_size\n        self.gamma = gamma\n        self.W_Q = nn.Parameter(torch.randn(hidden_size, head_size) / hidden_size)\n        self.W_K = nn.Parameter(torch.randn(hidden_size, head_size) / hidden_size)\n        self.W_V = nn.Parameter(torch.randn(hidden_size, self.v_dim) / hidden_size)\n\n        self.xpos = XPOS(head_size)\n\n    def forward(self, X):\n        \"\"\"\n        Parallel (default) representation of the retention mechanism.\n        X: (batch_size, sequence_length, hidden_size)\n        \"\"\"\n        sequence_length = X.shape[1]\n        D = self._get_D(sequence_length)\n\n        Q = (X @ self.W_Q)\n        K = (X @ self.W_K)\n\n        Q = self.xpos(Q)\n        K = self.xpos(K, downscale=True)\n\n        V = X @ self.W_V\n        Q, K, V ,D= Q.to(device), K.to(device), V.to(device),D.to(device)\n\n        ret = (Q @ K.permute(0, 2, 1)) * D.unsqueeze(0)\n\n        return ret @ V\n\n    def forward_recurrent(self, x_n, s_n_1, n):\n        \"\"\"\n        Recurrent representation of the retention mechanism.\n        x_n: (batch_size, 1, hidden_size)\n        s_n_1: (batch_size, hidden_size, v_dim)\n        \"\"\"\n\n        Q = (x_n @ self.W_Q)\n        K = (x_n @ self.W_K)\n\n        Q = self.xpos(Q, n+1)\n        K = self.xpos(K, n+1, downscale=True)\n\n        V = x_n @ self.W_V\n\n        # K: (batch_size, 1, hidden_size)\n        # V: (batch_size, 1, v_dim)\n        # s_n = gamma * s_n_1 + K^T @ V\n\n        s_n = self.gamma * s_n_1 + (K.transpose(-1, -2) @ V)\n\n        return (Q @ s_n), s_n\n\n    def forward_chunkwise(self, x_i, r_i_1, i):\n        \"\"\"\n        Chunkwise representation of the retention mechanism.\n        x_i: (batch_size, chunk_size, hidden_size)\n        r_i_1: (batch_size, hidden_size, v_dim)\n        \"\"\"\n        batch, chunk_size, _ = x_i.shape\n        D = self._get_D(chunk_size)\n        x_i = x_i.to(self.W_Q.device) \n        self.W_Q = self.W_Q.to(x_i.device)\n        self.W_K = self.W_K.to(x_i.device)\n        Q = (x_i @ self.W_Q)\n        K = (x_i @ self.W_K)\n\n        Q = self.xpos(Q, i * chunk_size)\n        K = self.xpos(K, i * chunk_size, downscale=True)\n        \n        V = x_i @ self.W_V\n\n        #print(r_i_1.shape)\n        r_i_1 = r_i_1[:K.shape[0]]  \n        r_i =(K.transpose(-1, -2) @ (V * D[-1].view(1, chunk_size, 1))) + (self.gamma ** chunk_size) * r_i_1\n\n        inner_chunk = ((Q @ K.transpose(-1, -2)) * D.unsqueeze(0)) @ V\n\n        #e[i,j] = gamma ** (i+1)\n        e = torch.zeros(batch, chunk_size, 1)\n\n        for _i in range(chunk_size):\n            e[:, _i, :] = self.gamma ** (_i + 1)\n\n        cross_chunk = (Q @ r_i_1) * e\n\n        return inner_chunk + cross_chunk, r_i\n\n    def _get_D(self, sequence_length):\n        n = torch.arange(sequence_length).unsqueeze(1)\n        m = torch.arange(sequence_length).unsqueeze(0)\n\n        # Broadcast self.gamma ** (n - m) with appropriate masking to set values where n < m to 0\n        D = (self.gamma ** (n - m)) * (n >= m).float()  #this results in some NaN when n is much larger than m\n        # fill the NaN with 0\n        D[D != D] = 0\n\n        return D\n\n\n\nclass MultiScaleRetention(nn.Module):\n    def __init__(self, hidden_size, heads, double_v_dim=False):\n        \"\"\"\n        Multi-scale retention mechanism based on the paper\n        \"Retentive Network: A Successor to Transformer for Large Language Models\"[https://arxiv.org/pdf/2307.08621.pdf]\n        \"\"\"\n        super(MultiScaleRetention, self).__init__()\n        self.hidden_size = hidden_size\n        self.v_dim = hidden_size * 2 if double_v_dim else hidden_size\n        self.heads = heads\n        assert hidden_size % heads == 0, \"hidden_size must be divisible by heads\"\n        self.head_size = hidden_size // heads\n        self.head_v_dim = hidden_size * 2 if double_v_dim else hidden_size\n\n        self.gammas = (1 - torch.exp(torch.linspace(math.log(1/32), math.log(1/512), heads))).detach().cpu().tolist()\n\n        self.swish = lambda x: x * torch.sigmoid(x)\n        self.W_G = nn.Parameter(torch.randn(hidden_size, self.v_dim) / hidden_size)\n        self.W_O = nn.Parameter(torch.randn(self.v_dim, hidden_size) / hidden_size)\n        self.group_norm = nn.GroupNorm(heads, self.v_dim)\n\n        self.retentions = nn.ModuleList([\n            SimpleRetention(self.hidden_size, gamma, self.head_size, double_v_dim) for gamma in self.gammas\n        ])\n\n    def forward(self, X):\n        \"\"\"\n        parallel representation of the multi-scale retention mechanism\n        \"\"\"\n\n        # apply each individual retention mechanism to X\n        Y = []\n        for i in range(self.heads):\n            Y.append(self.retentions[i](X))\n\n        Y = torch.cat(Y, dim=2)\n        Y_shape = Y.shape\n        Y = self.group_norm(Y.reshape(-1, self.v_dim)).reshape(Y_shape)\n\n        return (self.swish(X @ self.W_G) * Y) @ self.W_O\n\n    def forward_recurrent(self, x_n, s_n_1s, n):\n        \"\"\"\n        recurrent representation of the multi-scale retention mechanism\n        x_n: (batch_size, 1, hidden_size)\n        s_n_1s: (batch_size, heads, head_size, head_size)\n\n        \"\"\"\n\n        # apply each individual retention mechanism to a slice of X\n        Y = []\n        s_ns = []\n        for i in range(self.heads):\n            y, s_n = self.retentions[i].forward_recurrent(\n                x_n[:, :, :], s_n_1s[i], n\n            )\n            Y.append(y)\n            s_ns.append(s_n)\n\n        Y = torch.cat(Y, dim=2)\n        Y_shape = Y.shape\n        Y = self.group_norm(Y.reshape(-1, self.v_dim)).reshape(Y_shape)\n\n        return (self.swish(x_n @ self.W_G) * Y) @ self.W_O, s_ns\n\n    def forward_chunkwise(self, x_i, r_i_1s, i):\n        \"\"\"\n        chunkwise representation of the multi-scale retention mechanism\n        x_i: (batch_size, chunk_size, hidden_size)\n        r_i_1s: (batch_size, heads, head_size, head_size)\n        \"\"\"\n        batch, chunk_size, _ = x_i.shape\n\n        # apply each individual retention mechanism to a slice of X\n        Y = []\n        r_is = []\n        for j in range(self.heads):\n            y, r_i = self.retentions[j].forward_chunkwise(\n                x_i[:, :, :], r_i_1s[j], i\n            )\n            Y.append(y)\n            r_is.append(r_i)\n\n\n        Y = torch.cat(Y, dim=2)\n        Y_shape = Y.shape\n        Y = self.group_norm(Y.reshape(-1, self.v_dim)).reshape(Y_shape)\n        x_i = x_i.to(self.W_G.device)\n        return (self.swish(x_i @ self.W_G) * Y) @ self.W_O, r_is\n\nclass RetNet(nn.Module):\n    def __init__(self, layers, hidden_dim, ffn_size, heads, double_v_dim=False):\n        super(RetNet, self).__init__()\n        self.layers = layers\n        self.hidden_dim = hidden_dim\n        self.ffn_size = ffn_size\n        self.heads = heads\n        self.v_dim = hidden_dim * 2 if double_v_dim else hidden_dim\n\n        self.retentions = nn.ModuleList([\n            MultiScaleRetention(hidden_dim, heads, double_v_dim)\n            for _ in range(layers)\n        ])\n        self.ffns = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(hidden_dim, ffn_size),\n                nn.GELU(),\n                nn.Linear(ffn_size, hidden_dim)\n            )\n            for _ in range(layers)\n        ])\n        self.layer_norms_1 = nn.ModuleList([\n            nn.LayerNorm(hidden_dim)\n            for _ in range(layers)\n        ])\n        self.layer_norms_2 = nn.ModuleList([\n            nn.LayerNorm(hidden_dim)\n            for _ in range(layers)\n        ])\n\n    def forward(self, X):\n        \"\"\"\n        X: (batch_size, sequence_length, hidden_size)\n        \"\"\"\n        for i in range(self.layers):\n            Y = self.retentions[i](self.layer_norms_1[i](X)) + X\n\n            X = self.ffns[i](self.layer_norms_2[i](Y)) + Y\n\n        return X\n\n    def forward_recurrent(self, x_n, s_n_1s, n):\n        \"\"\"\n        X: (batch_size, sequence_length, hidden_size)\n        s_n_1s: list of lists of tensors of shape (batch_size, hidden_size // heads, hidden_size // heads)\n\n        \"\"\"\n        s_ns = []\n        for i in range(self.layers):\n            # list index out of range\n            o_n, s_n = self.retentions[i].forward_recurrent(self.layer_norms_1[i](x_n), s_n_1s[i], n)\n            y_n = o_n + x_n\n            s_ns.append(s_n)\n            x_n = self.ffns[i](self.layer_norms_2[i](y_n)) + y_n\n\n        return x_n, s_ns\n\n    def forward_chunkwise(self, x_i, r_i_1s, i):\n        \"\"\"\n        X: (batch_size, sequence_length, hidden_size)\n        r_i_1s: list of lists of tensors of shape (batch_size, hidden_size // heads, hidden_size // heads)\n\n        \"\"\"\n        r_is = []\n        for j in range(self.layers):\n            o_i, r_i = self.retentions[j].forward_chunkwise(self.layer_norms_1[j](x_i), r_i_1s[j], i)\n            y_i = o_i + x_i\n            r_is.append(r_i)\n            x_i = self.ffns[j](self.layer_norms_2[j](y_i)) + y_i\n\n        return x_i, r_is\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#batch_size = 512\n#train_dataloader = DataLoader(dataset=tr_loader, batch_size=batch_size, pin_memory=True, shuffle=True,drop_last=True, num_workers=2,)\n#test_dataloader = DataLoader(dataset=ts_loader, batch_size=batch_size, pin_memory=True, shuffle=True, num_workers=2,)\n#val_dataloader = DataLoader(dataset=val_loader, batch_size=batch_size, pin_memory=True, shuffle=True, num_workers=2,)\ndef train(network, tr_loader, val_loader, optimizer1, num_epochs=50, loss_weight=None, Y_t=None):\n    # 定义损失函数和优化器.\n\n    train_loss_list = []\n    train_acc_list = []\n    val_loss_list = []\n    val_acc_list = []\n    criterion1 = nn.CrossEntropyLoss()\n    # 定义在验证集上表现最好的模型准确率和损失.\n    best_val_acc = 0.0\n    best_val_loss = float('inf')\n    Best_epoch = 0\n    #定义存储最佳模型参数的变量.\n    best_model_params = None\n    # 开始训练模型.\n    for epoch in range(num_epochs):\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        network.train()\n        \n        for (x, y), idx in tr_loader:\n            x, y, y_t = x.to(device), y.flatten().to(device), Y_t[idx].flatten().to(device)\n            \n            optimizer1.opt_zero_grad()\n            #optimizer2.zero_grad()\n\n            l_2 = network(x)\n            \n            #print(l_2.shape)\n            #print(y.shape)\n            loss = args.lambda_cls*loss_cross_entropy(weight=loss_weight['org'])(l_2, y)\n            \n            _, predicted = torch.max(l_2.data, 1)\n            train_correct += (predicted == y).sum().item()\n            train_total += y.size(0)\n            # 累计训练损失\n            #scaler.scale(loss).backward()\n            #scaler.step(optimizer1)\n\n            #scaler.update()\n            loss.backward()\n            optimizer1.opt_step()\n            #optimizer2.step()\n            train_loss += loss.item()\n            print(\">\", end=\"\")\n        # 计算训练准确率和损失.\n        train_acc = 100.0 * train_correct / train_total\n        train_loss = train_loss / len(tr_loader)\n\n        # 在验证集上验证模型.\n        #val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        network.eval()\n        with torch.no_grad():\n            for iteration, batch in enumerate(zip(val_loader)):\n\n                x, y = batch[0]\n                x, y = x.to(device), y.flatten().to(device)\n\n                # 前向传播.\n                l_2 = network(x)\n                # 计算损失和准确率.\n\n                _, predicted = torch.max(l_2.data, 1)\n                val_correct += (predicted == y).sum().item()\n                val_total += y.size(0)\n\n                # 累计验证损失.\n                #val_loss += loss.item()\n\n        # 计算验证准确率和损失.\n        val_acc = 100.0 * val_correct / val_total\n        #val_loss = val_loss / len(val_loader)\n        if val_acc > best_val_acc:\n            Best_epoch = epoch + 1\n            best_val_acc = val_acc\n            #best_val_loss = val_loss\n            best_model_params = network.state_dict()\n\n        print()\n        # 打印训练和验证结果.\n        print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.2f}%,Val Acc: {:.2f}%'\n              .format(epoch + 1, num_epochs, train_loss, train_acc, val_acc))\n\n        # 保存训练和验证结果.\n        train_loss_list.append(train_loss)\n        train_acc_list.append(train_acc)\n        #val_loss_list.append(val_loss)\n        val_acc_list.append(val_acc)\n    print(\"The best epoch:\", Best_epoch, \"    Acc:\", best_val_acc)\n    network.load_state_dict(best_model_params)\n    # 返回训练和验证结果.\n    return network.eval(), train_loss_list, train_acc_list, val_acc_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Shrinkage_SE(nn.Module):\n    def __init__(self, channel, gap_size = 1):\n        super(Shrinkage_SE, self).__init__()\n        self.gap = nn.AdaptiveAvgPool1d(gap_size)\n        self.attention = nn.Sequential(\n            nn.Linear(channel, channel//16),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel//16, channel), \n            nn.Sigmoid()\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel, channel),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        \n        x_raw = x\n        x = torch.abs(x)\n\n        x_abs = x\n        x = self.gap(x)\n        x = torch.flatten(x, 1)\n        #attn = self.attention(x)##\n        # average = torch.mean(x, dim=1, keepdim=True)  #CS\n        average = x    #CW\n        x = self.fc(x)\n        x = torch.mul(average, x)\n        x = x.unsqueeze(2)\n        #attn = attn.unsqueeze(2)\n        # soft thresholding\n        sub = x_abs - x\n        #sub = x_abs - attn*x\n        \n        zeros = sub - sub\n        n_sub = torch.max(sub, zeros)\n        x = torch.mul(torch.sign(x_raw), n_sub)\n        return x\n    \n    \n########################\nclass ECA(nn.Module):\n    def __init__(self, channel, k_size=3):\n        super(ECA, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # feature descriptor on the global spatial information\n        y = self.avg_pool(x)\n        # Two different branches of ECA module\n        y = self.conv(y.transpose(-1, -2)).transpose(-1, -2)\n        # Multi-scale information fusion\n        y = self.sigmoid(y)\n        return x * y.expand_as(x)\n    \nclass Shrinkage_ECA(nn.Module):\n    def __init__(self, channel, gap_size):\n        super(Shrinkage_ECA, self).__init__()\n        self.gap = nn.AdaptiveAvgPool1d(gap_size)\n        self.attention = nn.Sequential(\n            nn.Linear(channel, channel//16),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel//16, channel), \n            nn.Sigmoid()\n        )\n        self.ECA = ECA(channel, k_size=3)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel, channel),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        \n        x_raw = x\n        x = torch.abs(x)\n        #attn = self.attention(x)##\n        attn = self.ECA(x)\n        x_abs = x\n        x = self.gap(x)\n        x = torch.flatten(x, 1)\n        #attn = self.attention(x)##\n        # average = torch.mean(x, dim=1, keepdim=True)  #CS\n        average = x    #CW\n        x = self.fc(x)\n        x = torch.mul(average, x)\n        x = x.unsqueeze(2)\n        #attn = attn.unsqueeze(2)\n        # soft thresholding\n        #sub = x_abs - x\n        sub = x_abs - attn*x\n        \n        zeros = sub - sub\n        n_sub = torch.max(sub, zeros)\n        x = torch.mul(torch.sign(x_raw), n_sub)\n        return x\n\ndef conv3(in_planes, out_planes, stride=1, groups=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, groups=groups, bias=False)\n\n\ndef conv1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n\n    return nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass SEModule(nn.Module):\n    def __init__(self, channels, reduction=2):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc1 = nn.Conv1d(channels, channels // reduction, kernel_size=1, padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv1d(channels // reduction, channels, kernel_size=1, padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input):\n        x = self.avg_pool(input)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return input * x\n\nclass EfficientChannelAttention(nn.Module):           # Efficient Channel Attention module\n    def __init__(self, c, b=1, gamma=2):\n        super(EfficientChannelAttention, self).__init__()\n        t = int(abs((math.log(c, 2) + b) / gamma))\n        k = t if t % 2 else t + 1\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.conv1 = nn.Conv1d(1, 1, kernel_size=k, padding=int(k/2), bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        y = x\n        x = self.avg_pool(x)\n        x = self.conv1(x.transpose(-1, -2)).transpose(-1, -2)\n        out = self.sigmoid(x)\n        return out+y\n\nclass GatedRes2NetBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, downsample=None,\n                 stride=1, scales=4, groups=1, mod='ECA', norm_layer=None):\n        super(GatedRes2NetBottleneck, self).__init__()\n        if planes * groups % scales != 0:\n            raise ValueError('Planes must be divisible by scales')\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm1d\n        bottleneck_planes = groups * planes\n        self.conv1 = conv1(inplanes, bottleneck_planes, stride)\n        self.bn1 = norm_layer(bottleneck_planes)\n        self.conv2 = nn.ModuleList([conv3(bottleneck_planes // scales,\n                                          bottleneck_planes // scales,\n                                          groups=groups) for _ in range(scales - 1)])\n        self.SE = nn.ModuleList([EfficientChannelAttention(bottleneck_planes // scales) for _ in range(scales - 1)])\n        self.bn2 = nn.ModuleList([norm_layer(bottleneck_planes // scales)\n                                  for _ in range(scales - 1)])\n        self.judge = nn.ModuleList([conv1(bottleneck_planes+2*bottleneck_planes // scales,\n                                          bottleneck_planes // scales\n                                          ) for _ in range(scales - 2)])\n        self.tanh = nn.Tanh()\n\n\n        self.conv3 = conv1(bottleneck_planes, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.model=mod\n        if mod == 'ECA':\n            self.mod = EfficientChannelAttention(self.expansion*planes)\n        elif mod == 'SE':\n            self.mod = SEModule(self.expansion*planes)\n        \n        self.downsample = downsample\n        self.stride = stride\n        self.scales = scales\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        xs = torch.chunk(out, self.scales, 1)\n        ys = []\n        before = []\n        att = []\n        for s in range(self.scales):\n            if s == 0:\n                ys.append(xs[s])\n            \n            elif s == 1:\n                ys.append(self.relu(self.bn2[s - 1](self.conv2[s - 1](xs[s]))))\n                before.append(ys[-1])\n                ys[-1] = (self.SE[s - 1](ys[-1]))\n            else:\n                #gate = self.tanh(self.judge[s - 2](torch.cat([out,xs[s],ys[-1]],dim=1)))\n                #ys.append(self.relu(self.bn2[s - 1](self.conv2[s - 1](xs[s] + gate * ys[-1]))))\n                ys.append(self.relu(self.bn2[s - 1](self.conv2[s - 1](xs[s]))))\n                before.append(ys[-1])\n                att.append(before[-2]+ys[-2])\n                ys[-1] = self.SE[s - 1](ys[-1]+att[-1])\n        \n\n        out = torch.cat(ys, 1)\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.model =='SE':\n            out = self.mod(out)\n        elif self.model == 'ECA':\n            x = self.mod(out)\n            out= out+x\n\n        if self.downsample is not None:\n            identity = self.downsample(identity)\n\n        out = out + identity\n        out = self.relu(out)\n\n        return out\n\nclass GatedFCN(nn.Module):\n    def __init__(self, layers, maxlength=6, groups=1,\n                  width=10,scales=6,mod='ECA', norm_layer=None,inplanes=128):\n        super(GatedFCN, self).__init__()\n        planes = [int(width * scales * 2 ** i) for i in range(4)]\n\n        self.pre=inplanes\n        self.inplanes=planes[0]\n        self.maxlength=maxlength\n\n        self.pre=conv1(inplanes,planes[0])\n\n        self.layer1 = self._make_layer(GatedRes2NetBottleneck, planes[0], layers[0], scales=scales, groups=groups, mod=mod,\n                                       norm_layer=norm_layer)\n\n        self.layer2 = self._make_layer(GatedRes2NetBottleneck, planes[1], layers[1], stride=2, scales=scales, groups=groups,\n                                       mod=mod, norm_layer=norm_layer)\n\n        self.layer3 = self._make_layer(GatedRes2NetBottleneck, planes[2], layers[2], stride=2, scales=scales, groups=groups,\n                                       mod=mod, norm_layer=norm_layer)\n\n        self.layer4 = self._make_layer(GatedRes2NetBottleneck, planes[3], layers[3], stride=2, scales=scales, groups=groups,\n                                       mod=mod, norm_layer=norm_layer)\n\n        self.final=conv1(1024,args.seq_length)\n        self.roi=nn.AdaptiveAvgPool1d(output_size=1)\n        self.mapping=conv1(in_planes=1024,out_planes=maxlength)\n        self.dropout1 = nn.Dropout(p=0.1)\n        self.dropout2 = nn.Dropout(p=0.15)\n        self.dropout3 = nn.Dropout(p=0.2)\n        self.dropout4 = nn.Dropout(p=0.25)\n\n    def _make_layer(self, block, planes, block_num, stride=1, scales=4, groups=1, mod='ECA', norm_layer=None):\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm1d\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, downsample, stride=stride, scales=scales, groups=groups, mod=mod,\n                            norm_layer=norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, block_num):\n            layers.append(block(self.inplanes, planes, scales=scales, groups=groups, mod=mod, norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self,x):\n        b, l, t = x.shape\n        \n        \n        x=  x.to(device)\n\n        x=  self.pre(x)\n        x1 = self.dropout1(x)\n        x1=  self.layer1(x1)\n        \n        \n        x2 = self.dropout2(x1)\n        x2 = self.layer2(x2)\n        \n        x3 = self.dropout3(x2)\n        x3 = self.layer3(x3)\n        output = x3.reshape(b, args.seq_length, -1)\n\n        #print(output.shape)\n        #x5=  self.roi(x4)\n        #output=self.mapping(x5)\n        #output=output.squeeze()\n\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def g1(mod='None',inplanes=3):\n    return GatedFCN([1,2,2,1],mod=mod,inplanes=inplanes)\ndef g2(mod='None',inplanes=3):\n    return GatedFCN([1,2,2,0],mod=mod,inplanes=inplanes)\ndef g3(mod='None',inplanes=3):\n    return GatedFCN([2,3,4,1],mod=mod,inplanes=inplanes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import copy\nimport math\n#from utils import *\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.nn as nn\n\n\nclass Classifier(nn.Module):\n    def __init__(self, in_f, out_f):\n        '''\n        Classify FE feature\n        '''\n        super(Classifier, self).__init__()\n        self.linear_1 = nn.Linear(in_f, out_f)  # 创建一个nn.Linear实例，并将其赋值给self.linear_1，输入的参数为in_f和out_f\n\n    def forward(self, x):\n        x = self.linear_1(x)  # 将输入张量x通过self.linear_1处理得到输出张量\n        return x\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=args.seq_length):\n\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        if args.scheme == 'M_O': max_len = max_len // 2 + 1 \n        pe = torch.zeros(max_len, d_model) \n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(\n            1)  \n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (\n                    -math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        if args.scheme == 'M_O': pe = torch.cat(\n            [pe, pe.flip(dims=(0, 1))[1:]])\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n\nclass MSNN_Feature_Embedding_Two_Way(nn.Module):\n    def __init__(self):\n        super(MSNN_Feature_Embedding_Two_Way, self).__init__()\n        # 定义卷积、可分离卷积\n        conv = lambda in_f, out_f, kernel, s=None: nn.Sequential(nn.Conv1d(in_f, out_f, (kernel,), stride=s),\n                                                                 nn.BatchNorm1d(out_f), nn.LeakyReLU())\n        sepconv_same = lambda in_f, out_f, kernel: nn.Sequential(\n            nn.Conv1d(in_f, out_f, (kernel,), padding=int(kernel / 2), groups=in_f),\n            nn.Conv1d(out_f, out_f, (1,)), nn.BatchNorm1d(out_f), nn.LeakyReLU())\n\n        self.conv_A_0 = conv(1, 4, 5, 1)\n        self.sepconv_A_1 = sepconv_same(4, 16, 9)\n        self.sepconv_A_2 = sepconv_same(16, 32, 5)\n        self.sepconv_A_3 = sepconv_same(32, 64, 3) \n\n        self.conv_B_0 = conv(1, 4, 10, 1)  \n        self.sepconv_B_1 = sepconv_same(4, 16, 9) \n        self.sepconv_B_2 = sepconv_same(16, 32, 5) \n        self.sepconv_B_3 = sepconv_same(32, 64, 3) \n        \n        self.conv_C_0 = conv(1, 4, 20, 1)  \n        self.sepconv_C_1 = sepconv_same(4, 16, 9) \n        self.sepconv_C_2 = sepconv_same(16, 32, 5) \n        self.sepconv_C_3 = sepconv_same(32, 64, 3) \n        self.ATT = EfficientChannelAttention(112)\n        self.gap = nn.AdaptiveAvgPool1d(args.mha_length)  \n        self.pe = PositionalEncoding(112,\n                                     max_len=args.mha_length)\n        self.mha_ff_A = nn.TransformerEncoderLayer(112, args.mha_head,\n                                                   dim_feedforward=448)\n        self.mha_ff_B = nn.TransformerEncoderLayer(112, args.mha_head,\n                                                   dim_feedforward=448) \n        self.mha_ff_C = nn.TransformerEncoderLayer(112, args.mha_head,\n                                                   dim_feedforward=448) \n\n    def seq_trans(self, func, x):\n        # 输入形状：[B, F, T]\n        return func(x.permute(-1, 0, 1)).permute(1, -1, 0)\n\n    def one_way(self, conv_0, sepconv_1, sepconv_2, sepconv_3, x, mha_ff=None,att = None):\n        b, l, t = x.shape\n        x = x.reshape(-1, 1, t) #[B*F,1,T]\n        x = conv_0(x)\n        x = sepconv_1(x)\n        x1 = x\n        x = sepconv_2(x) \n        x2 = x\n        x = sepconv_3(x) \n        x3 = x\n        x = self.gap(torch.cat([x1, x2, x3], 1))\n        t = self.ATT(x)\n        if att !=None:\n            x = self.ATT(x+att)\n        att = t+x\n        x = x + self.seq_trans(self.pe, x) \n        x = x.permute(-1, 0, 1) #[T,B*F,F]\n        x = mha_ff(x).permute(1, -1, 0) \n        x = x.reshape(b, l, *x.shape[-2:]) \n\n        return x , att\n    def one_way0(self, conv_0, sepconv_1, sepconv_2, sepconv_3, x, mha_ff=None,att = None):\n        b, l, t = x.shape\n        x = x.reshape(-1, 1, t) #[B*F,1,T]\n        x = conv_0(x)\n        x = sepconv_1(x)\n        x1 = x\n        x = sepconv_2(x) \n        x2 = x\n        x = sepconv_3(x) \n        x3 = x\n        x = self.gap(torch.cat([x1, x2, x3], 1))\n        x = x + self.seq_trans(self.pe, x) \n        x = x.permute(-1, 0, 1) #[T,B*F,F]\n        x = mha_ff(x).permute(1, -1, 0) \n        x = x.reshape(b, l, *x.shape[-2:]) \n\n        return x\n\n    def forward(self, x):\n        \n        x_A = self.one_way0(self.conv_A_0, self.sepconv_A_1, self.sepconv_A_2, self.sepconv_A_3, x,\n                           mha_ff=self.mha_ff_A) \n        x_B ,att= self.one_way(self.conv_B_0, self.sepconv_B_1, self.sepconv_B_2, self.sepconv_B_3, x,\n                           mha_ff=self.mha_ff_B)\n        x_C ,att= self.one_way(self.conv_C_0, self.sepconv_C_1, self.sepconv_C_2, self.sepconv_C_3, x,\n                           mha_ff=self.mha_ff_C,att = att)\n        x = torch.cat((x_A, x_B,x_C), dim=-2) \n        return x\n#############################################################################################################################\n\nclass Context_Encoder(nn.Module):\n    def __init__(self, f, h):\n        super(Context_Encoder, self).__init__()\n\n        self.biLSTM = nn.LSTM(f, h, num_layers=args.lstm_layers, bidirectional=True,batch_first=True)\n        self.biLSTM = nn.LSTM(f, h ,num_layers=args.lstm_layers, dropout=0.5, bidirectional=True,batch_first=True)\n        self.dropout_1 = nn.Dropout() \n        self.dropout_2 = nn.Dropout() \n        self.MR1 = RetNet(layers=1, hidden_dim=f, ffn_size=h, heads=16, double_v_dim=False)\n        \n    def forward(self, x):  # [B, L, F]\n        \n        h, _ = self.biLSTM(x)  \n        h = self.dropout_1(h) \n        h = h+x\n        h = self.dropout_2(h) \n        return h\n    def forward_ret(self, x): \n        h = self.MR1(x)  \n        h = self.dropout_1(h) \n        h = h+x\n        h = self.dropout_2(h) \n        return h\n#############################################################################################################################\n\nclass Model(nn.Module):\n    def __init__(self, x, num_classes=17):\n        super(Model, self).__init__()\n        self.FE = MSNN_Feature_Embedding_Two_Way().to(device)\n        with torch.no_grad():\n            b, l, f, t = self.FE(x).shape \n            feature_size = f * t  \n            embedding_size = int(feature_size / 2)\n        self.res1 =  g1(mod='None',inplanes=113).to(device)\n        self.Context_Encoder = Context_Encoder(feature_size, embedding_size).to(device) \n        with torch.no_grad(): \n            x = self.FE(x)\n            x = x.flatten(start_dim=2)\n            feature_size2 = self.Context_Encoder(x).shape[-1]\n        self.project_f = nn.Linear(feature_size2,768) \n        self.dropout = nn.Dropout()\n        self.cls = Classifier(feature_size2, 17)\n\n    def forward(self, x):\n        x = x.to(device)\n        #print(x.shape)\n        #b, l, f = x.shape  # 计算输入张量x的形状\n        #x = x.view(-1,113, 30)\n\n        #x = self.res1(x)\n        \n        x = self.FE(x)\n        b, l, f, t = x.shape  # 计算输入张量x的形状\n        x = x.view(b,l, -1)\n\n        h = network.Context_Encoder.forward_ret(x)\n        l_2 = network.cls(h.to(device)) \n        l_2 = l_2.flatten(end_dim=1)\n\n        return l_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_t = {'tr': Y_tr_t, 'val': Y_val_t, 'ts': Y_ts_t}\nnetwork = Model(x=tr_loader.dataset.tensors[0][:2].to(device)).to(device)\nimport time\nopt = Optimizer(network)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\nmodel, train_loss_list, train_acc_list, val_acc_list = \\\n    train(network=network, tr_loader=tr_loader, val_loader=val_loader, num_epochs=50, optimizer1=opt,\n          loss_weight=loss_weight_dict, Y_t=Y_t['tr'])\nend_time = time.time()\nuse_time = end_time - start_time\nprint(\"Train and val complete in {:.0f}m {:.0f}s\".format(use_time // 60, use_time % 60))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef plot_loss_and_acc(train_loss_list, train_acc_list, val_acc_list):\n    # 绘制训练和验证损失\n    plt.figure(figsize=(10, 5))\n    plt.plot(train_loss_list, label='train_loss')\n    #plt.plot(val_loss_list, label='val_loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\n    # 绘制训练和验证准确率\n    plt.figure(figsize=(10, 5))\n    plt.plot(train_acc_list, label='train_acc')\n    plt.plot(val_acc_list, label='val_acc')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n\n\nplot_loss_and_acc(train_loss_list, train_acc_list, val_acc_list)\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\ndef test_final(model, test_dataloader,loss_weight = 0.007):\n    # 将模型设置为测试模式.\n    model.eval()\n\n       # 定义损失函数和优化器.\n    criterion1 = nn.CrossEntropyLoss()\n    #criterion2 = CenterLoss(6, 6).to(device)\n\n    loss_weight=0.007\n    # 在测试集上测试模型.\n    test_loss = 0.0\n    test_correct = 0\n    test_total = 0\n    y_true = []\n    y_pred = []\n    num_classes = 6\n    conf_matrix = np.zeros((num_classes, num_classes), dtype=int)  # 创建混淆矩阵\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(test_dataloader):\n            # 将输入和标签数据转换为Tensor并放到GPU上（如果有的话）.\n            #inputs = inputs.to(device)\n            #labels = labels.to(device)\n            outputs= model(inputs.type(torch.float32))\n            labels = labels.long()\n            labels = (labels.clone().detach()-1).to(device).squeeze()\n\n            # 前向传播.\n            #outputs = model(inputs)\n            pre_lab = torch.argmax(outputs, 1)\n            # 计算损失和准确率.\n            loss = criterion1(outputs, labels)\n            #loss = criterion1(outputs, labels) + loss_weight * criterion2(labels, outputs)\n            _, predicted = torch.max(\n                                     outputs.data, 1)\n            test_correct += (predicted == labels).sum().item()\n            test_total += labels.size(0)\n\n            # 累计测试损失.\n            test_loss += loss.item()\n            y_true.extend(labels.tolist())\n            y_pred.extend(pre_lab.tolist())\n            conf_matrix += confusion_matrix(labels.cpu(), pre_lab.cpu(), labels=range(num_classes))\n    report = classification_report(y_true, y_pred,digits=4)\n    g_mean = np.sqrt(np.diag(conf_matrix) / np.sum(conf_matrix, axis=1))\n    g_mean = np.mean(g_mean)\n\n    # 更新分类报告\n    report += '\\nG-mean: {:.4f}'.format(g_mean)\n    # 计算测试准确率和损失.\n    test_acc = 100.0 * test_correct / test_total\n    test_loss = test_loss / len(test_dataloader)\n\n    # 打印测试结果.\n    print('Test Loss: {:.4f}, Test Acc: {:.2f}%'.format(test_loss, test_acc))\n    print(report)\n    # 返回测试结果.\n    return test_loss, test_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_acc=test_final(model, ts_dataloader,loss_weight = 0.007)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.font_manager as font_manager\nimport shutil\nimport matplotlib\nimport matplotlib.font_manager as fm\n\n\nfont_dirs = '/kaggle/input/timesttf/times.ttf'\n#font_paths = fm.findSystemFonts() # 查系统中可用的字体X件路径\nfm.fontManager.addfont(font_dirs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\nfrom matplotlib.ticker import PercentFormatter\nclass DrawConfusionMatrix:\n    def __init__(self, labels_name, normalize=True):\n        self.normalize = normalize\n        self.labels_name = labels_name\n        self.num_classes = len(labels_name)\n        self.matrix = np.zeros((self.num_classes, self.num_classes), dtype=\"float32\")\n        self.class_counts_matrix = np.zeros((self.num_classes, self.num_classes), dtype=\"int32\")\n\n    def update(self, labels, predicts):\n        for predict, label in zip(labels, predicts):\n            self.matrix[label, predict] += 1\n            self.class_counts_matrix[label, predict] += 1\n\n    def getMatrix(self, normalize=True):\n        if normalize:\n            per_sum = self.matrix.sum(axis=1)  # 计算每行的和，用于百分比计算\n            for i in range(self.num_classes):\n                self.matrix[i] = (self.matrix[i] / per_sum[i])  # 百分比转换\n            self.matrix = np.around(self.matrix, 4)  # 保留4位小数点\n            self.matrix[np.isnan(self.matrix)] = 0.0  # 可能存在NaN，将其设为0\n        return self.matrix\n\n    def drawMatrix(self):\n        self.matrix = self.getMatrix(self.normalize)\n        font = FontProperties(family='serif', style='normal', weight='normal', size=10)\n        plt.figure(dpi=480)\n        plt.imshow(self.matrix, cmap=plt.cm.Blues)  # 仅画出颜色格子，没有值\n        plt.title(\"WISDM\", fontproperties=font)  # 标题\n        plt.xlabel(\"Predict label\", fontproperties=font)\n        plt.ylabel(\"Truth label\", fontproperties=font)\n\n        plt.yticks(range(self.num_classes), self.labels_name, fontproperties=font)  # y轴标签\n        plt.xticks(range(self.num_classes), self.labels_name, rotation=30, fontproperties=font)  # x轴标签\n        thresh = self.matrix.max() / 2.\n\n        for x in range(self.num_classes):\n            for y in range(self.num_classes):\n                count = self.class_counts_matrix[y, x]\n                value = str(format('%.2f' % float(self.matrix[y, x] * 100.00))) + '%'  # 数值处理\n                #text = f\"{count}\\n{value}\"\n                text = count\n                plt.text(x, y-0.13, text, verticalalignment='center',\n                         horizontalalignment='center',\n                         color=\"white\" if self.matrix[y, x] > thresh else \"black\", fontproperties=font, fontsize=8)  # 写值\n                plt.text(x, y+0.13, value, verticalalignment='center',\n                         horizontalalignment='center',\n                         color=\"white\" if self.matrix[y, x] > thresh else \"black\", fontproperties=font, fontsize=8)  # 写值\n\n        plt.tight_layout()  # 自动调整子图参数，使之填充整个图像区域\n\n        #色条\n        plt.colorbar(format=PercentFormatter(xmax=1, decimals=0, symbol='%', is_latex=False))\n        plt.savefig('./ConfusionMatrix.png', bbox_inches='tight')  # bbox_inches='tight'可确保标签信息显示全\n        plt.show()\n\n\ndef printMatrix(test_loader, model):\n    #labels_name = ['Downstairs', 'Jogging', 'Sitting', 'Standing', 'Upstairs', 'Walking']\n    #labels_name=[\"Walking\", \"Upstairs\", \"Downstairs\", \"Sitting\", \"Standing\", \"Laying\"]\n    labels_name = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18']\n    drawconfusionmatrix = DrawConfusionMatrix(labels_name=labels_name)  # 实例化\n    for index, (imgs, labels) in enumerate(test_loader, 1):\n        labels_pd = model(imgs.float())\n        predict_np = np.argmax(labels_pd.cpu().detach().numpy(), axis=-1)  # array([0,5,1,6,3,...],dtype=int64)\n        labels_np = labels.numpy()  # array([0,5,0,6,2,...],dtype=int64)\n        drawconfusionmatrix.update(labels_np, predict_np)  # 将新批次的predict和label更新（保存）\n\n    drawconfusionmatrix.drawMatrix()  # 根据所有predict和label，画出混淆矩阵\n\n    confusion_mat = drawconfusionmatrix.getMatrix()  # 你也可以使用该函数获取混淆矩阵(ndarray)\n    print(confusion_mat)\n\nprint(\"开始绘制混淆矩阵\")\nprintMatrix(val_dataloader, model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}